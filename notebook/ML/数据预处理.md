# 数据预处理

[文章链接](https://blog.csdn.net/stay_zezo/article/details/107591277)

## 获取数据
本文用的数据集是100种植物种类数据集，用于分类任务

数据集下载地址：[100种树叶数据集](https://wws.lanzous.com/ilvyQezms1a)

## 加载数据集
本文用到了其中的data_Tex_64.txt，使用pandas加载数据

```python
import numpy as np
import pandas as pd
from sklearn import preprocessing

#加载数据和标签
df = pd.read_csv('./data/100 leaves plant species/data_Tex_64.txt')
origin_feature  = df.iloc[:, 1:]
origin_labels = df.iloc[:, 0]
```

## 特征值和标签处理
数据处理常使用sklearn.preprocessing的子模块来实现

1. 归一化(最小-最大规范化)：将数据缩放到0~1之间,使用MinMaxScaler模块

    归一化公式

    <img src="https://i.loli.net/2021/05/04/TPA6oI23VCWKpk4.png" alt="归一化" style="zoom:150%;" />

    

    ```python
    #归一化
    min_max_scaler = preprocessing.MinMaxScaler()
    feature = min_max_scaler.fit_transform(origin_feature)
    
    #归一化还原
    orgin_feature = min_max_scaler.inverse_transform(feature)
    ```

2. 标准化（z-score规范化）：将数据转化成均值为0，方差为1。使用 StandardScaler模块。先求出全部数据的均值和方差，再进行计算。最后的结果均值为0，方差是1。

    标准化公式

    <img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWM0LnpoaW1nLmNvbS92Mi1jOTJkMGY1YzViNjEzN2Y4YmJiMDY4M2M0MzU2NTg3M19iLnBuZw?x-oss-process=image/format,png" alt="img" style="zoom:150%;" />

    **注意：当原始数据并不符合高斯分布的话，标准化后的数据效果并不好，这时就不要使用标准化**

    ```python
    #标准化
    s_scaler = preprocessing.StandardScaler()
    feature = s_scaler.fit_transform(origin_feature)
    
    #数据还原
    origin_feature = s_scaler.inverse_transform(feature)
    ```

3. 标签编码（数字化）

    很多标签是文本形式，这种格式没法用于训练模型，所以需要将文本标签转成数字标签。用到LabelEncoder模块。下图就是上面数据集的原始标签。

    ![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWMyLnpoaW1nLmNvbS92Mi1lMWRmNzRlMzZhMmZiY2NkNmMzYzA0NDZlY2I4OGY2NV9iLnBuZw?x-oss-process=image/format,png)

    标签类型

    ```python
    #标签数字化
    le = preprocessing.LabelEncoder()
    labels =le.fit_transform(origin_labels).astype(np.int64)
    
    #标签还原
    labels = le.inverse_transform(labels)
    ```

4. one-hot编码（特征和标签）：One-Hot编码是分类变量作为二进制向量的表示。这首先要求将分类值映射到整数值。然后，每个整数值被表示为二进制向量，除了整数的索引之外，它都是零值，它被标记为1。

    上面数据集的标签是 [ 0 0 0 ... 99 99 99]。由于标签是一维，所以需要加【】变成二维，再转置成列向量。

     ![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWM0LnpoaW1nLmNvbS92Mi1hNWQ5ZDJiOWRkY2E5ZGRmYTAzMTg3NjVhMDU1ZjkyM19iLnBuZw?x-oss-process=image/format,png)


    标签由一维的行向量准换成二维列向量


    one-hot编码可以由sklearn.preprocessing import OneHotEncoder和pandas里的get_dummies实现，OneHotEncoder只能处理数值型而get_dummies可以对其他特殊字符进行编码

    1. 使用preprocessing.onehotEncoder()编码

        ```python
        #one-hot编码
        labels = np.array([labels]).T
        enc = preprocessing.OneHotEncoder()
        labels = enc.fit_transform(labels).toarray()
        
        #还原
        origin_labels = enc.inverse_transform(labels)
        
        
        #查看多少类别
        le.classes_
        ```

    2. 使用pandas的get_dummies模块对特征值进行one-hot编码

    假如1.txt的数据如下：

    ![img](https://img-blog.csdnimg.cn/20200727025345339.png)

    对Cabin和Embarked列执行one-hot编码得到新得到df2，再跟去掉Cabin和Embarked的原始df1合并得到编码后的df

    ```python
    #用pandas里的get_dummies可以对字符特征变量Sex','Embarked'处理
    import pandas as pd
    
    df1 = pd.read_csv('./data/1.txt')
    
    df2=pd.get_dummies(df1[['Carbin','Embarked']],drop_first=True) 
    
    #借助concat函数把编码变量合并到原来数据
    df=pd.concat([df1.drop(['Carbin','Embarked'],axis=1), df2],axis=1) 
    ```

## 特征二值化

定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0

![aHR0cHM6Ly9waWMzLnpoaW1nLmNvbS92Mi1kY2QzY2FhMjM5NjJhM2FlY2I1YzYzZTdmNzVkOTgzNl9iLnBuZw](https://i.loli.net/2021/05/04/jAUSzNfcPXt7rCI.png)

二值化公式

```python
#特征二值化
binarizer = preprocessing.Binarizer(threshold=0.2)
feature = binarizer.transform(origin_feature)
```

## 标签二值化：

```python
#标签二值化
lb = preprocessing.LabelBinarizer()
labels = lb.fit_transform(origin_labels)
```

## 划分数据集：

```python
#划分数据集为训练集和测试集
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test= train_test_split(origin_feature, 
                                                origin_labels,test_size=0.3,random_state=0)

```

## 制作可迭代的pytorch数据集 （选看）
方便pytorch框架操作数据

```python
#划分数据集
import torch
import numpy as np
import torch.utils.data as Data
from sklearn.model_selection import train_test_split

x_train, x_test, y_train,y_test = train_test_split(feature, labels, test_size=0.25)

#制作pytorch识别的数据集
train_dataset = Data.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train))
test_dataset = Data.TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test))


#制作可迭代的数据集
train_iter = Data.DataLoader(dataset = train_dataset,batch_size = batch_size,
                             shuffle = True, num_workers = 2)
test_iter = Data.DataLoader(dataset = test_dataset, batch_size= batch_size,
                           shuffle = True, num_workers = 2) 
```


制作完可迭代的数据集后，就可以方便的训练模型。

```python
#循环一次，获得一批量的数据
for X, y in train_iter:
    print(X)
    print(y)
    break
```