#  交叉熵

交叉熵是信息论中的一个重要概念，它的大小表示两个概率分布之间的差异，可以通过最小化交叉熵来得到目标概率分布的近似分布。本文主要介绍了交叉熵及其相关概念以及交叉熵损失函数的应用。

------

为了理解交叉熵，首先要了解下面这几个概念。

## **自信息**

信息论的基本想法是，一个不太可能的事件发生了的话，要比一个非常可能发生的事件提供更多的信息。

如果想通过这种想法来量化信息，需要满足以下性质：

1. 非常可能发生的事件信息量比较少，并且极端情况下，确定能够发生的事件没有信息量。
2. 较不可能发生的事件具有更高的信息量。
3. 独立事件应具有增量的信息。例如，投掷的硬币两次正面朝上传递的信息量，应该是投掷一次硬币正面朝上的信息量的两倍。

根据以上三点，定义**自信息**(self-information)：

![img](https://pic4.zhimg.com/80/v2-b1d933107e222fdc99545e4c462b882f_1440w.jpg)

在本文中，用log来表示自然对数，其底数为e。

## **香农熵**

设 ![[公式]](https://www.zhihu.com/equation?tex=X) 是一个有限个值的离散随机变量，其概率分布为：

![img](https://pic3.zhimg.com/80/v2-23104cbb69ee618a41956603334c22a2_1440w.png)

对于该离散随机变量，用自信息的期望来量化整个概率分布中的不确定性总量：

![img](https://pic1.zhimg.com/80/v2-28e62ec3462d5298c4da5fed7089cdc4_1440w.jpg)

这里的H(X)定义为随机变量 ![[公式]](https://www.zhihu.com/equation?tex=X) 的**香农熵**(Shannon entropy)，香农熵只依赖于 ![[公式]](https://www.zhihu.com/equation?tex=X) 的分布，而与 ![[公式]](https://www.zhihu.com/equation?tex=X) 的取值无关，所以香农熵也记作H(P)。



下面借助抛硬币的例子来简单理解一下香农熵。

在抛硬币时，假设正面朝上的概率为*p，*反面朝上的概率为1-*p*，即 P(*X*=正) = *p*, P(*X*=反) = 1-*p，*香农熵大小为：

![[公式]](https://www.zhihu.com/equation?tex=\quad\quad\quad\quad+H(p)+%3D+-p+\cdot+logp+-+(1-p)+\cdot+log(1-p))

H(*p*)随概率*p*变化的曲线为

![img](https://pic3.zhimg.com/80/v2-81ae252badc3c59e1d76315079073996_1440w.jpg)

从上图中可以看到，当正面朝上的概率*p*=0或1时，这时我们完全可以确定抛硬币的结果，此时香农熵H(*p*)=0，随机变量完全没有不确定性。当*p*=0.5时，正面和反面出现的概率相同，我们完全无法确定结果，此时香农熵取值最大，随机变量不确定性也最大。这说明了更接近确定性的分布具有较低的香农熵，而更接近均匀分布的分布（不确定性最大）具有较高的香农熵。

看到这里是不是明白了一些呢，说白了，香农熵就是代表了事件发生的不确定性，不确定性越大，香农熵越大，不确定性越小，香农熵越小。

## **KL散度(相对熵)**

KL散度(Kullback-Leibler divergence)，也叫做相对熵。若随机变量 ![[公式]](https://www.zhihu.com/equation?tex=X) 有两个单独的概率分布P(*X*)和Q(*X*)，可以用相对熵来衡量这两个分布的差异，**相对熵**的定义如下：

![img](https://pic1.zhimg.com/80/v2-e75331faea08c855636e7263e00981bc_1440w.jpg)

相对熵是非负的，它的大小可以用来衡量两个分布之间的差异，当且仅当P和Q具有完全相同的分布时，相对熵取值为0。

需要注意的是，相对熵并不是对称的，对于某些P和Q， ![[公式]](https://www.zhihu.com/equation?tex=D_{KL}(P||Q)\neq+D_{KL}(Q||P)+)

------

概念介绍完了，终于要进入正题了！

Are you ready?



## **交叉熵**

通过上面的介绍，相信聪明的同学已经发现可以通过最小化相对熵来用分布Q逼近分布P(目标概率分布)。首先我们对相对熵公式进行变形：

![img](https://pic2.zhimg.com/80/v2-0af0ae95ea73522e1718be0350f6929d_1440w.jpg)

这里的H(P,Q)就是**交叉熵**(cross-entropy)，它的表达式为：

![img](https://pic3.zhimg.com/80/v2-4d8aa0d70095cf466578afd5a020e7f6_1440w.jpg)

对于确定的概率分布P，它的香农熵H(*P*)是一个常数，所以要对相对熵 ![[公式]](https://www.zhihu.com/equation?tex=D_{KL}(P||Q)) 进行最小化，只需对交叉熵H(*P,Q*)做最小化处理即可。

通过最小化交叉熵，就可以得到分布P的近似分布，这也是为什么可以用交叉熵作为网络的损失函数。



最后简单介绍一下交叉熵在机器学习中的应用。

## **交叉熵损失函数**

交叉熵损失函数常用于分类问题中，下面以图像分类问题来举例说明。

为了计算网络的loss，模型的输出要确保归一化到0到1之间，二分类问题通常使用sigmoid函数来进行归一化，多分类问题通常使用softmax函数来归一化。

> 假设我们需要对数字1，2，3进行分类，它们的label依次为：
> [1,0,0]， [0,1,0]， [0,0,1]
> 当输入的图像为数字1时，它的输出和label为：
> [0.3,0.4,0.3] , [1,0,0]
> 接下来我们就可以利用交叉熵计算网络的 ![[公式]](https://www.zhihu.com/equation?tex=loss+%3D+-%281%2Alog%280.3%29+%2B+0+%2B+0%29+%3D1.20)
> 随着训练次数的增加，模型的参数得到优化，这时的输出变为：[0.8,0.1,0.1]
> 则 ![[公式]](https://www.zhihu.com/equation?tex=loss+%3D+-%281%2Alog%280.8%29+%2B+0+%2B+0%29+%3D+0.22)

可以发现loss由1.20减小为0.22，而判断输入图像为数字1的概率由原本的0.3增加为0.8，说明训练得到的概率分布越来越接近真实的分布，这样就大大的提高了预测的准确性。