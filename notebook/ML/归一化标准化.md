# 数据预处理

## 归一化 Normalization

>归一化一般是将数据映射到指定的范围，用于去除不同维度数据的量纲以及量纲单位。
>
>常见的映射范围有 [0, 1] 和 [-1, 1] ，最常见的归一化方法就是 **Min-Max 归一化**：
>
>![[公式]](https://www.zhihu.com/equation?tex=x_{new}%3D\frac{x-x_{min}}{x_{max}-x_{min}})
>
>举个例子，我们判断一个人的身体状况是否健康，那么我们会采集人体的很多指标，比如说：身高、体重、红细胞数量、白细胞数量等。
>
>一个人身高 180cm，体重 70kg，白细胞计数 ![[公式]](https://www.zhihu.com/equation?tex=7.50×10^{9}%2FL) ，etc.
>
>衡量两个人的状况时，白细胞计数就会起到主导作用从而**遮盖住其他的特征**，归一化后就不会有这样的问题。

## 标准化 Normalization

1. **Z-Score 标准化**

> ![[公式]](https://www.zhihu.com/equation?tex=x_{new}%3D\frac{x-\mu+}{\sigma+})
>
> 其中 ![[公式]](https://www.zhihu.com/equation?tex=\mu) 是样本数据的**均值（mean）**， ![[公式]](https://www.zhihu.com/equation?tex=\sigma) 是样本数据的**标准差（std）**。
>
> ![img](https://pic3.zhimg.com/80/v2-ee0280ea470db277509e95efce1991f6_720w.jpg)
>
> 
>
> 上图则是一个散点序列的标准化过程：原图->减去均值->除以标准差。
>
> 显而易见，变成了一个**均值为 0 ，方差为 1 的分布**，下图通过 Cost 函数让我们更好的理解标准化的作用。
>
> 为什么要标准化?**标准化后可以更加容易地得出最优参数** ![[公式]](https://www.zhihu.com/equation?tex=w) **和** ![[公式]](https://www.zhihu.com/equation?tex=b) **以及计算出** ![[公式]](https://www.zhihu.com/equation?tex=J(w%2Cb)) **的最小值，从而达到加速收敛的效果**。

2. **batch normalization**

> 我们知道数据预处理做标准化可以加速收敛，同理，在神经网络使用标准化也可以**加速收敛**，而且还有如下好处：
>
> - 具有正则化的效果（Batch Normalization reglarizes the model）
> - 提高模型的泛化能力（Be advantageous to the generalization of network）
> - 允许更高的学习速率从而加速收敛（Batch Normalization enables higher learning rates）
>
> 其原理是**利用正则化减少内部相关变量分布的偏移（Reducing Internal Covariate Shift）**，从而**提高了算法的鲁棒性**。 ![[公式]](https://www.zhihu.com/equation?tex=^{[2]})
>
> Batch Normalization 由两部分组成，第一部分是**缩放与平移（scale and shift）**，第二部分是**训练缩放尺度和平移的参数（train a BN Network）**，算法步骤如下：
>
> ![img](https://pic4.zhimg.com/80/v2-3df73c23287a7ed941024e7630b385f7_720w.jpg)

3. 正则化 Regularization

> **正则化主要用于避免过拟合的产生和减少网络误差。**
>
> 
>
> 正则化一般具有如下形式：
>
> ![[公式]](https://www.zhihu.com/equation?tex=J(w%2Cb)%3D+\frac{1}{m}+\sum_{i%3D1}^{m}L(f(x)%2Cy))+λR(f))
>
> 其中，第 1 项是**经验风险**，第 2 项是**正则项**， ![[公式]](https://www.zhihu.com/equation?tex=λ≥0) 为调整两者之间关系的系数。
>
> 第 1 项的经验风险较小的模型可能较复杂（有多个非零参数），这时第 2 项的模型复杂度会较大。
>
> 常见的有正则项有 **L1 正则** 和 **L2 正则** ，其中 **L2 正则** 的控制过拟合的效果比 **L1 正则** 的好。
>
> **正则化的作用是选择经验风险与模型复杂度同时较小的模型**。 ![[公式]](https://www.zhihu.com/equation?tex=^{[3]})
>
> 常见的有正则项有 **L1 正则** 和 **L2 正则** 以及 **Dropout** ，其中 **L2 正则** 的控制过拟合的效果比 **L1 正则** 的好。
>
> ## ![[公式]](https://www.zhihu.com/equation?tex=L_{p}) **范数**
>
> 为什么叫 L1 正则，有 L1、L2 正则 那么有没有 L3、L4 之类的呢？
>
> 首先我们补一补课， ![[公式]](https://www.zhihu.com/equation?tex=L_{p}) 正则的 L 是指 ![[公式]](https://www.zhihu.com/equation?tex=L_%7Bp%7D) 范数，其定义为：
>
> ![[公式]](https://www.zhihu.com/equation?tex=L_{0}) 范数： ![[公式]](https://www.zhihu.com/equation?tex=\left+\|+w+\right+\|_{0}+%3D+\%23(i)\+with+\+x_{i}+\neq+0) *（非零元素的个数）*
>
> ![[公式]](https://www.zhihu.com/equation?tex=L_{1}) 范数： ![[公式]](https://www.zhihu.com/equation?tex=\left+\|+w+\right+\|_{1}+%3D+\sum_{i+%3D+1}^{d}\lvert+x_i\rvert) *（每个元素绝对值之和）*
>
> ![[公式]](https://www.zhihu.com/equation?tex=L_{2}) 范数： ![[公式]](https://www.zhihu.com/equation?tex=\left+\|+w+\right+\|_{2}+%3D+\Bigl(\sum_{i+%3D+1}^{d}+x_i^2\Bigr)^{1%2F2}) *（欧氏距离）*
>
> ![[公式]](https://www.zhihu.com/equation?tex=L_%7Bp%7D) 范数： ![[公式]](https://www.zhihu.com/equation?tex=\left+\|+w+\right+\|_{p}+%3D+\Bigl(\sum_{i+%3D+1}^{d}+x_i^p\Bigr)^{1%2Fp})
>
> 在机器学习中，若使用了 ![[公式]](https://www.zhihu.com/equation?tex=\lVert+w\rVert_p) 作为正则项，我们则说该机器学习任务**引入了** ![[公式]](https://www.zhihu.com/equation?tex=L_%7Bp%7D) **正则项**。
>
> ![img](https://pic3.zhimg.com/80/v2-af9e02d00d9e37cfa0920583cab6d5b6_720w.jpg)

## **L1 正则 Lasso regularizer**

![[公式]](https://www.zhihu.com/equation?tex=J(w%2Cb)%3D\frac{1}{m}+\sum_{i%3D1}^{m}L(\hat{y}%2Cy)%2B\frac{\lambda+}{m}\left+\|+w+\right+\|_{1})

- 凸函数，不是处处可微分
- 得到的是稀疏解（最优解常出现在顶点上，且顶点上的 w 只有很少的元素是非零的）



## **L2 正则 Ridge Regularizer / Weight Decay**

![[公式]](https://www.zhihu.com/equation?tex=J(w%2Cb)%3D\frac{1}{m}+\sum_{i%3D1}^{m}L(\hat{y}%2Cy)%2B\frac{\lambda+}{2m}\left+\|+w+\right+\|^{2}_{2})

- 凸函数，处处可微分
- 易于优化



## **Dropout**

**Dropout 主要用于神经网络，其原理是使神经网络中的某些神经元随机失活，让模型不过度依赖某一神经元，达到增强模型鲁棒性以及控制过拟合的效果。**

除此之外，Dropout 还有多模型投票等功能，若有兴趣可以拜读[这篇论文](https://link.zhihu.com/?target=https%3A//www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)**。**

![img](https://pic1.zhimg.com/80/v2-c03db8ee45cb5b8d29cc157d0023aa14_720w.jpg)

## 独热编码 one hot

- 对于离散的特征基本就是按照**one-hot（独热）**编码，该离散特征有多少取值，就用多少维来表示该特征。
- 独热码，在英文文献中称做 one-hot code, 直观来说就是有多少个状态就有多少比特，而且只有一个比特为1，其他全为0的一种码制
- 